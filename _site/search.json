[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Stat and CS student at Swarthmore College"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nGaussian Mixture Models and Expectation-Maximization\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2023\n\n\nAbraham Porschet\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Stat-61-Final-Project/Stat-61-Final-Project.html",
    "href": "posts/Stat-61-Final-Project/Stat-61-Final-Project.html",
    "title": "Gaussian Mixture Models and Expectation-Maximization",
    "section": "",
    "text": "The Expectation Maximization (E-M) Algorithm is an iterative approach to find maximum likelihood estimates for latent variables (since the things we want to maximize are only indirectly available). It is comprised of an estimation step, which tries to estimate the unknown variables, and a maximization step, which then tries to optimize the parameters of the model to better explain the data.\nThe unknown parameters are sometimes written as \\(\\phi\\) or \\(\\Theta\\), and we can call the latent, “nuisance,” variables \\(J\\), and the observed data \\(U\\). So, from above, the process can be roughly seen as \\[ \\Theta^* = \\operatorname*{argmax}_{\\Theta} \\sum_{J\\in\\mathcal{J}^n} P(\\Theta, J|U) \\] Since this shows us maximizing the posterior probability of parameters \\(\\Theta\\) given our data and we are summing over \\(J\\) in order to marginalize out our latent variables (Dellaert, 2002).\nThis process was first rigorously defined on the exponential family, where the probability density functions take the form \\[ f(x|\\phi) = b(x)\\exp(\\phi t(x)^T/a(\\phi))\\]\nwhere \\(\\phi\\) is a \\(1\\times r\\) parameter vector and \\(t(x)\\) is a \\(1\\times r\\) vector of sufficient statistics for the data. Our “natural parameter” for these exponential distributions is given by some \\(r\\times r\\) linear transformation.\nTo run the algorithm on this example, we first enter the expectation step, and take \\(t^{(p)}=E[t(x)|y,\\phi^{(p)}]\\) with the \\((p)\\) denoting the \\(p^{th}\\) cycle of the algorithm, trying to estimate the vector of sufficient statistics for the exponential distribution.\nThe maximization step, is then taking the equation \\(E[t(x)|y,\\phi^{(p)}]=t^{(p)}\\) and we call the solution to this equation \\(\\phi^{(p+1)}\\). We then plug in \\(\\phi^{(p+1)}\\) to the expectation step and keep iterating.\nThis looks nice, and seems like it could work, but if you are anything like me, you might wonder how we decide when to stop. Possibly the coolest part of this algorithm is that it actually converges to a local maximum every time (Dempster et al. 1976). Since it always converges to a local maximum, it means that if we “guess” a decent parameter space to start off the algorithm, we will converge to what is likely to be a very solid set of estimates.\nIn Dempster, Laird, and Rubin’s seminal paper Maximum Likelihood via the ‘EM’ Algorithm, they enumerate the process detailed above, proved the convergence, and later on, proposed that E-M could be used to in finite clusters. This foreshadowed the most common usage of the algorithm, clustering, or more specifically, dividing unlabeled data into nice clusters. For example, if we know that our raw data is comprised of unique groups represented by different probability distributions, we can use the E-M algorithm to change the parameters for the estimated distributions of these groups to maximize the probability that the data belongs to the proposed clustering.\n\n\nOne of the most common usages of expectation maximization, and specifically clustering, is Gaussian Mixture Modeling (GMM) (Hasselblad 1966). This process is essentially assuming that each group you are trying to sort out is represented by a normal distribution. This is often a very convenient technique to use because things often actually do follow normal distributions because of the central limit theorem (everyone’s favorite statistics theorem) and because once we have clusters that are normal, it is much easier to do inference on the clusters and talk about them with people who don’t know as much about statistics because gaussian distributions are so well understood.\nGMMs are used to observe clusters everywhere. They are used to create customer archetypes in retail, to better understand the different ways people shop, they are used in medical scenarios in order to identify types of tumors for cancer detection. From this little sample of use cases, it is pretty obvious that this is a really powerful (and pretty cool) use for a powerful algorithm.\n\n\n\n\n#importing necessary packages\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt \nimport pandas as pd \nimport numpy as np\nfrom numpy import random\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport seaborn as sns \nfrom sklearn.metrics import silhouette_score, silhouette_samples\nfrom sklearn.decomposition import PCA\nimport requests\n\nTo show how effective this algorithm can be, I am going to make a set of five blobs of data, each with a center, and then I will show how accurately the algorithm can cluster the data into the blobs that created the underlying data.\n\n#set a random seed so that we actually get clusters that kind of look like separate clusters\nrandom.seed(195)\nx, _ = make_blobs(n_samples=450, centers=5, cluster_std=1.84)\nplt.figure(figsize=(8, 6))\nplt.scatter(x[:,0], x[:,1])\nplt.show() \n\n\n\n\nNow I’m going to fit the algorithm with the prior understanding that the data is made of five clusters of approximately normal data.\n\n\n{'covariance_type': 'full',\n 'init_params': 'kmeans',\n 'max_iter': 100,\n 'means_init': None,\n 'n_components': 5,\n 'n_init': 1,\n 'precisions_init': None,\n 'random_state': None,\n 'reg_covar': 1e-06,\n 'tol': 0.001,\n 'verbose': 0,\n 'verbose_interval': 10,\n 'warm_start': False,\n 'weights_init': None}\n\n\nThis code here fits the model and lets it learn from the data, in the next plot, I will plot the centers that the data came up with, and then on the plot after that I will plot the boundaries for the clusters that the algorithm came up with.\n\ncenters = gm.means_\nplt.figure(figsize=(8, 6))\nplt.scatter(x[:,0], x[:,1], label=\"data\")\nplt.scatter(centers[:,0], centers[:,1],c='r', label=\"centers\")\nplt.legend()\nplt.show() \n\n\n\n\n\npred = gm.predict(x)    \n\ndf = pd.DataFrame({'x':x[:,0], 'y':x[:,1], 'label':pred})\ngroups = df.groupby('label')\n\nig, ax = plt.subplots()\nfor name, group in groups:\n    ax.scatter(group.x, group.y, label=name)\n\nax.legend()\nplt.show() \n\n\n\n\nThe accuracy is really fantastic. It plots the centers exactly where I would have, and then is able to pick which dots it belongs to which groups with great accuracy.\nNow if we consider a dataset with millions of points and possibly hundreds or thousands of dimensions, this allows for very sensitive anomaly detection for genetic disorders by clustering genes or proteins it helps us see just how helpful this could be.\n\n\n\n\nBasketball as a sport is changing. Players like Stephen Curry have changed perceptions around what a point guard is supposed to do, Nikola Jokic is reinventing the center position, and some teams are playing with centers who are shorter than 6’5. Another even bigger change is the advent of extremely tall players playing seemingly positionless basketball, the trend started by players such as Kevin Durant and Kristaps Porzingis, and continued by younger players like Chet Holmgren and Victor Wembanyama.\nPeople are playing basketball differently. To effectively understand the game, the old labels of point guard, shooting guard, center, power forward, and small forward don’t seem to suffice, which means that we want to find new labels for positions in order to regroup players.\nThis seems to be a problem uniquely well suited to clustering. I plan on looking at a few things, how the clusters of players in the modern NBA compare to the positions that players are assigned to. Secondly, I am curious if the NBA has become more specialized, i.e. if there are more than five positions, and players are acquired and used for more specific purposes than in the past. To answer these questions, even just a little bit, I plan on clustering players from the 2022 season (stats acquired from Basketball Reference) and answering my first question using that data and model, and then taking in data from the 1986 season (that was fun and long enough ago to measure change) and looking at the difference in number of clusters for those seasons.\n\nurls = ['https://www.basketball-reference.com/leagues/NBA_2022_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2023_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2021_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2019_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2018_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2017_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2016_per_game.html']\ndef scrape_basketball_data(url):\n    html = urlopen(url)\n    org_html = BeautifulSoup(html)\n    org_html.findAll('tr', limit=2)\n\n    headers = [th.getText() for th in org_html.findAll('tr', limit=2)[0].findAll('th')]\n    headers = headers[1:]\n\n    rows = org_html.findAll('tr')[1:]\n    player_stats = [[td.getText() for td in rows[i].findAll('td')] for i in range(len(rows))]\n    data = pd.DataFrame(player_stats, columns = headers)\n    \n    return data\n\ndef aggregate_data_from_urls(urls):\n    all_dataframes = []\n    for url in urls:\n        df = scrape_basketball_data(url)\n        all_dataframes.append(df)\n    \n    combined_data = pd.concat(all_dataframes, ignore_index=True)\n    combined_data = combined_data.apply(pd.to_numeric, errors='ignore')\n    \n    aggregated_data = combined_data.groupby('Player', as_index=False).mean()\n    return aggregated_data\n\ndf = aggregate_data_from_urls(urls)\ndf = df[df['MP']&gt;=10]\ndf\n\n/var/folders/6_/c68mr5wx1xz8wp1g177rf99r0000gn/T/ipykernel_21839/1310375822.py:25: FutureWarning:\n\nThe default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n\n\n\n\n\n\n\n\n\n\nPlayer\nAge\nG\nGS\nMP\nFG\nFGA\nFG%\n3P\n3PA\n...\nFT%\nORB\nDRB\nTRB\nAST\nSTL\nBLK\nTOV\nPF\nPTS\n\n\n\n\n3\nAJ Griffin\n19.000000\n72.000000\n12.000000\n19.500000\n3.400000\n7.400000\n0.465000\n1.400000\n3.600000\n...\n0.894000\n0.500000\n1.600000\n2.100000\n1.000000\n0.600000\n0.200000\n0.600000\n1.200000\n8.900000\n\n\n4\nAaron Brooks\n32.000000\n55.333333\n0.333333\n11.933333\n1.833333\n4.533333\n0.403333\n0.666667\n1.900000\n...\n0.764333\n0.266667\n0.766667\n1.033333\n1.700000\n0.333333\n0.066667\n0.833333\n1.400000\n4.800000\n\n\n5\nAaron Gordon\n23.777778\n59.666667\n54.111111\n29.355556\n5.200000\n10.966667\n0.477111\n1.200000\n3.555556\n...\n0.683556\n1.711111\n4.544444\n6.266667\n2.733333\n0.744444\n0.688889\n1.644444\n1.955556\n13.777778\n\n\n6\nAaron Harrison\n22.000000\n11.666667\n1.000000\n11.233333\n0.766667\n3.133333\n0.179333\n0.366667\n1.900000\n...\n0.560667\n0.200000\n1.100000\n1.333333\n0.633333\n0.433333\n0.066667\n0.166667\n1.300000\n2.600000\n\n\n8\nAaron Holiday\n24.500000\n50.833333\n7.333333\n15.466667\n2.216667\n5.250000\n0.422333\n0.733333\n1.916667\n...\n0.848333\n0.333333\n1.316667\n1.633333\n2.116667\n0.633333\n0.166667\n0.966667\n1.433333\n6.033333\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1218\nZhaire Smith\n19.000000\n6.000000\n2.000000\n18.500000\n2.300000\n5.700000\n0.412000\n1.000000\n2.700000\n...\n0.750000\n0.500000\n1.700000\n2.200000\n1.700000\n0.300000\n0.300000\n1.000000\n1.300000\n6.700000\n\n\n1220\nZiaire Williams\n20.500000\n49.500000\n17.500000\n18.450000\n2.700000\n6.050000\n0.439500\n0.950000\n3.250000\n...\n0.777500\n0.400000\n1.700000\n2.100000\n0.950000\n0.500000\n0.200000\n0.850000\n1.700000\n6.900000\n\n\n1221\nZion Williamson\n21.000000\n45.000000\n45.000000\n33.100000\n10.100000\n16.600000\n0.609500\n0.200000\n0.650000\n...\n0.706000\n2.350000\n4.750000\n7.100000\n4.150000\n1.000000\n0.600000\n3.050000\n2.200000\n26.500000\n\n\n1223\nÁlex Abrines\n24.000000\n58.000000\n5.333333\n16.533333\n1.766667\n4.666667\n0.381667\n1.266667\n3.533333\n...\n0.889667\n0.266667\n1.200000\n1.433333\n0.533333\n0.500000\n0.133333\n0.433333\n1.700000\n5.333333\n\n\n1225\nÖmer Aşık\n30.400000\n27.000000\n16.600000\n13.360000\n0.800000\n1.760000\n0.438000\n0.000000\n0.000000\n...\n0.355200\n0.960000\n2.840000\n3.820000\n0.300000\n0.200000\n0.280000\n0.660000\n1.400000\n2.040000\n\n\n\n\n905 rows × 27 columns\n\n\n\nThere are a few instances of the same player showing multiple times in the dataframe since people were traded and played for different teams throughout the season, so I took the averages of all of their values to create a set of stats for the season for them.\n\ndf=df.drop(columns=['Age', 'G', 'GS', '2P', '3P', 'TRB', 'FT','PF', 'FG', 'MP'], axis=1)\n\nSo now that we have this data, I will take all of the features besides position, age, team, games played, games started (As well as some irrelevant features, i.e. those that are just linear combinations of other features) and will use them to create clusters so we can start to draw some conclusions. We should feel pretty good about modeling the clusters as gaussian since there are over eight hundred players that played in 2022 which means we should feel alright about assuming normality across each predictor, especially since we have data across 6 seasons.\n\nn_components = np.arange(1, 15)\nmodels = [GaussianMixture(n, covariance_type='full', random_state=0).fit(X)\n          for n in n_components]\n\nplt.plot(n_components, [m.bic(X) for m in models], label='BIC')\nplt.plot(n_components, [m.aic(X) for m in models], label='AIC')\nplt.legend(loc='best')\nplt.xlabel('n_components');\n\n\n\n\nThis code here, uses metrics Bayesian Information Criterion (BIC) and Aikake Information Criterion (AIC), to see which number of clusters would best tell us about the data. The lower the value the better. Both of the metrics are based on the likelihood function for the potential mixture models and the main difference between them is that BIC punishes models with more parameters more than AIC, as we can see from the plot, since the BIC is minimized between 4 and 5 and AIC continues to decrease as the number of components reaches 50. The AIC is equal to \\(2k-2\\ln(L)\\) and the BIC is equal to \\(k\\ln(n)-2\\ln(L)\\) where \\(k\\) is the number of parameters. So as the likelihood that the model proposed (dependent on the number of clusters) has a higher likelihood of explaining the data, the AIC and BIC both decrease.\nThis graph essentially says that the model that explains the data the best without overfitting is between 4 and 5 clusters, since that is where BIC is at a minimum.\n\nmodel = GaussianMixture(n_components=4, random_state=0).fit(X)\n\n\npreds = pd.Series(model.predict(X))\nX[\"Cluster\"] = preds\nX.dropna()\n\n\n\n\n\n\n\n\nFGA\nFG%\n3PA\n3P%\n2PA\n2P%\neFG%\nFTA\nFT%\nORB\nDRB\nAST\nSTL\nBLK\nTOV\nPTS\nCluster\n\n\n\n\n3\n7.400000\n0.465000\n3.600000\n0.390000\n3.800000\n0.536000\n0.560000\n0.700000\n0.894000\n0.500000\n1.600000\n1.000000\n0.600000\n0.200000\n0.600000\n8.900000\n3.0\n\n\n4\n4.533333\n0.403333\n1.900000\n0.362333\n2.633333\n0.433667\n0.480000\n0.600000\n0.764333\n0.266667\n0.766667\n1.700000\n0.333333\n0.066667\n0.833333\n4.800000\n0.0\n\n\n5\n10.966667\n0.477111\n3.555556\n0.325222\n7.422222\n0.544667\n0.529222\n3.222222\n0.683556\n1.711111\n4.544444\n2.733333\n0.744444\n0.688889\n1.644444\n13.777778\n1.0\n\n\n6\n3.133333\n0.179333\n1.900000\n0.169667\n1.233333\n0.202333\n0.227667\n0.966667\n0.560667\n0.200000\n1.100000\n0.633333\n0.433333\n0.066667\n0.166667\n2.600000\n3.0\n\n\n8\n5.250000\n0.422333\n1.916667\n0.380333\n3.316667\n0.448000\n0.491833\n1.050000\n0.848333\n0.333333\n1.316667\n2.116667\n0.633333\n0.166667\n0.966667\n6.033333\n3.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n867\n7.400000\n0.437000\n2.500000\n0.353000\n4.900000\n0.480000\n0.497000\n1.000000\n0.905000\n0.100000\n1.600000\n2.400000\n0.300000\n0.100000\n1.400000\n8.200000\n1.0\n\n\n868\n10.050000\n0.435000\n2.650000\n0.314000\n7.400000\n0.477500\n0.476000\n2.100000\n0.756500\n0.400000\n2.650000\n3.950000\n1.500000\n0.450000\n2.150000\n11.150000\n3.0\n\n\n869\n7.480000\n0.521000\n2.700000\n0.314400\n4.800000\n0.590600\n0.578600\n1.120000\n0.859400\n0.280000\n2.020000\n3.760000\n0.800000\n0.140000\n0.720000\n9.360000\n0.0\n\n\n870\n6.844444\n0.633444\n0.133333\n0.107111\n6.711111\n0.644222\n0.635000\n3.222222\n0.661556\n1.711111\n3.033333\n1.366667\n0.466667\n0.655556\n0.933333\n10.888889\n3.0\n\n\n871\n5.328571\n0.438429\n2.314286\n0.326143\n3.014286\n0.522286\n0.508571\n1.942857\n0.777286\n0.671429\n2.657143\n1.100000\n0.442857\n0.328571\n1.000000\n7.214286\n2.0\n\n\n\n\n621 rows × 17 columns\n\n\n\n\nY = X.drop('Cluster', axis = 1)\ncluster_assignments = model.predict(Y)\n\n# Get the cluster centers from the fitted model\ncluster_centers = model.means_  # This will give the centers of each cluster\n\n# Calculate distances between each player and the cluster centers\ndistances_to_centers = np.linalg.norm(Y - cluster_centers[cluster_assignments], axis=1)\n\n# Find the indices of players closest to each cluster center\nclosest_players_indices = np.argmin(distances_to_centers, axis=0)\n\n# Get the details of players closest to each cluster center\nclosest_players = Y.iloc[closest_players_indices]\n\n\nX_numeric = X.select_dtypes(include='number')\n\n# Scale the numeric columns\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_numeric)\n\n# Convert back to DataFrame (keeping the column names)\nX_scaled_df = pd.DataFrame(X_scaled, columns=X_numeric.columns)\n\n# Add the cluster assignments to your scaled data\nX_scaled_df['Cluster'] = cluster_assignments\n\n# Melt the DataFrame to create a long-form dataset for violin plot\nmelted_data_scaled = X_scaled_df.melt(id_vars='Cluster', var_name='Statistic', value_name='Value')\n\n# Plotting separate violin plots for each statistic\ng = sns.FacetGrid(melted_data_scaled, col='Statistic', col_wrap=3, height=4)\ng.map(sns.violinplot, 'Cluster', 'Value', palette='Set3')\ng.set_xticklabels(rotation=45)\ng.set_titles('{col_name}')\nplt.tight_layout()\nplt.show()\n\n/Users/abeporschet/anaconda3/lib/python3.11/site-packages/seaborn/axisgrid.py:712: UserWarning:\n\nUsing the violinplot function without specifying `order` is likely to produce an incorrect plot."
  },
  {
    "objectID": "posts/Stat-61-Final-Project/Stat-61-Final-Project.html#gaussian-mixture-modeling",
    "href": "posts/Stat-61-Final-Project/Stat-61-Final-Project.html#gaussian-mixture-modeling",
    "title": "Gaussian Mixture Models and Expectation-Maximization",
    "section": "",
    "text": "One of the most common usages of expectation maximization, and specifically clustering, is Gaussian Mixture Modeling (GMM) (Hasselblad 1966). This process is essentially assuming that each group you are trying to sort out is represented by a normal distribution. This is often a very convenient technique to use because things often actually do follow normal distributions because of the central limit theorem (everyone’s favorite statistics theorem) and because once we have clusters that are normal, it is much easier to do inference on the clusters and talk about them with people who don’t know as much about statistics because gaussian distributions are so well understood.\nGMMs are used to observe clusters everywhere. They are used to create customer archetypes in retail, to better understand the different ways people shop, they are used in medical scenarios in order to identify types of tumors for cancer detection. From this little sample of use cases, it is pretty obvious that this is a really powerful (and pretty cool) use for a powerful algorithm."
  },
  {
    "objectID": "posts/Stat-61-Final-Project/Stat-61-Final-Project.html#example-code",
    "href": "posts/Stat-61-Final-Project/Stat-61-Final-Project.html#example-code",
    "title": "Gaussian Mixture Models and Expectation-Maximization",
    "section": "",
    "text": "#importing necessary packages\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt \nimport pandas as pd \nimport numpy as np\nfrom numpy import random\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport seaborn as sns \nfrom sklearn.metrics import silhouette_score, silhouette_samples\nfrom sklearn.decomposition import PCA\nimport requests\n\nTo show how effective this algorithm can be, I am going to make a set of five blobs of data, each with a center, and then I will show how accurately the algorithm can cluster the data into the blobs that created the underlying data.\n\n#set a random seed so that we actually get clusters that kind of look like separate clusters\nrandom.seed(195)\nx, _ = make_blobs(n_samples=450, centers=5, cluster_std=1.84)\nplt.figure(figsize=(8, 6))\nplt.scatter(x[:,0], x[:,1])\nplt.show() \n\n\n\n\nNow I’m going to fit the algorithm with the prior understanding that the data is made of five clusters of approximately normal data.\n\n\n{'covariance_type': 'full',\n 'init_params': 'kmeans',\n 'max_iter': 100,\n 'means_init': None,\n 'n_components': 5,\n 'n_init': 1,\n 'precisions_init': None,\n 'random_state': None,\n 'reg_covar': 1e-06,\n 'tol': 0.001,\n 'verbose': 0,\n 'verbose_interval': 10,\n 'warm_start': False,\n 'weights_init': None}\n\n\nThis code here fits the model and lets it learn from the data, in the next plot, I will plot the centers that the data came up with, and then on the plot after that I will plot the boundaries for the clusters that the algorithm came up with.\n\ncenters = gm.means_\nplt.figure(figsize=(8, 6))\nplt.scatter(x[:,0], x[:,1], label=\"data\")\nplt.scatter(centers[:,0], centers[:,1],c='r', label=\"centers\")\nplt.legend()\nplt.show() \n\n\n\n\n\npred = gm.predict(x)    \n\ndf = pd.DataFrame({'x':x[:,0], 'y':x[:,1], 'label':pred})\ngroups = df.groupby('label')\n\nig, ax = plt.subplots()\nfor name, group in groups:\n    ax.scatter(group.x, group.y, label=name)\n\nax.legend()\nplt.show() \n\n\n\n\nThe accuracy is really fantastic. It plots the centers exactly where I would have, and then is able to pick which dots it belongs to which groups with great accuracy.\nNow if we consider a dataset with millions of points and possibly hundreds or thousands of dimensions, this allows for very sensitive anomaly detection for genetic disorders by clustering genes or proteins it helps us see just how helpful this could be."
  },
  {
    "objectID": "posts/Stat-61-Final-Project/Stat-61-Final-Project.html#the-nba",
    "href": "posts/Stat-61-Final-Project/Stat-61-Final-Project.html#the-nba",
    "title": "Gaussian Mixture Models and Expectation-Maximization",
    "section": "",
    "text": "Basketball as a sport is changing. Players like Stephen Curry have changed perceptions around what a point guard is supposed to do, Nikola Jokic is reinventing the center position, and some teams are playing with centers who are shorter than 6’5. Another even bigger change is the advent of extremely tall players playing seemingly positionless basketball, the trend started by players such as Kevin Durant and Kristaps Porzingis, and continued by younger players like Chet Holmgren and Victor Wembanyama.\nPeople are playing basketball differently. To effectively understand the game, the old labels of point guard, shooting guard, center, power forward, and small forward don’t seem to suffice, which means that we want to find new labels for positions in order to regroup players.\nThis seems to be a problem uniquely well suited to clustering. I plan on looking at a few things, how the clusters of players in the modern NBA compare to the positions that players are assigned to. Secondly, I am curious if the NBA has become more specialized, i.e. if there are more than five positions, and players are acquired and used for more specific purposes than in the past. To answer these questions, even just a little bit, I plan on clustering players from the 2022 season (stats acquired from Basketball Reference) and answering my first question using that data and model, and then taking in data from the 1986 season (that was fun and long enough ago to measure change) and looking at the difference in number of clusters for those seasons.\n\nurls = ['https://www.basketball-reference.com/leagues/NBA_2022_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2023_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2021_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2019_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2018_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2017_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2016_per_game.html']\ndef scrape_basketball_data(url):\n    html = urlopen(url)\n    org_html = BeautifulSoup(html)\n    org_html.findAll('tr', limit=2)\n\n    headers = [th.getText() for th in org_html.findAll('tr', limit=2)[0].findAll('th')]\n    headers = headers[1:]\n\n    rows = org_html.findAll('tr')[1:]\n    player_stats = [[td.getText() for td in rows[i].findAll('td')] for i in range(len(rows))]\n    data = pd.DataFrame(player_stats, columns = headers)\n    \n    return data\n\ndef aggregate_data_from_urls(urls):\n    all_dataframes = []\n    for url in urls:\n        df = scrape_basketball_data(url)\n        all_dataframes.append(df)\n    \n    combined_data = pd.concat(all_dataframes, ignore_index=True)\n    combined_data = combined_data.apply(pd.to_numeric, errors='ignore')\n    \n    aggregated_data = combined_data.groupby('Player', as_index=False).mean()\n    return aggregated_data\n\ndf = aggregate_data_from_urls(urls)\ndf = df[df['MP']&gt;=10]\ndf\n\n/var/folders/6_/c68mr5wx1xz8wp1g177rf99r0000gn/T/ipykernel_21839/1310375822.py:25: FutureWarning:\n\nThe default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n\n\n\n\n\n\n\n\n\n\nPlayer\nAge\nG\nGS\nMP\nFG\nFGA\nFG%\n3P\n3PA\n...\nFT%\nORB\nDRB\nTRB\nAST\nSTL\nBLK\nTOV\nPF\nPTS\n\n\n\n\n3\nAJ Griffin\n19.000000\n72.000000\n12.000000\n19.500000\n3.400000\n7.400000\n0.465000\n1.400000\n3.600000\n...\n0.894000\n0.500000\n1.600000\n2.100000\n1.000000\n0.600000\n0.200000\n0.600000\n1.200000\n8.900000\n\n\n4\nAaron Brooks\n32.000000\n55.333333\n0.333333\n11.933333\n1.833333\n4.533333\n0.403333\n0.666667\n1.900000\n...\n0.764333\n0.266667\n0.766667\n1.033333\n1.700000\n0.333333\n0.066667\n0.833333\n1.400000\n4.800000\n\n\n5\nAaron Gordon\n23.777778\n59.666667\n54.111111\n29.355556\n5.200000\n10.966667\n0.477111\n1.200000\n3.555556\n...\n0.683556\n1.711111\n4.544444\n6.266667\n2.733333\n0.744444\n0.688889\n1.644444\n1.955556\n13.777778\n\n\n6\nAaron Harrison\n22.000000\n11.666667\n1.000000\n11.233333\n0.766667\n3.133333\n0.179333\n0.366667\n1.900000\n...\n0.560667\n0.200000\n1.100000\n1.333333\n0.633333\n0.433333\n0.066667\n0.166667\n1.300000\n2.600000\n\n\n8\nAaron Holiday\n24.500000\n50.833333\n7.333333\n15.466667\n2.216667\n5.250000\n0.422333\n0.733333\n1.916667\n...\n0.848333\n0.333333\n1.316667\n1.633333\n2.116667\n0.633333\n0.166667\n0.966667\n1.433333\n6.033333\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1218\nZhaire Smith\n19.000000\n6.000000\n2.000000\n18.500000\n2.300000\n5.700000\n0.412000\n1.000000\n2.700000\n...\n0.750000\n0.500000\n1.700000\n2.200000\n1.700000\n0.300000\n0.300000\n1.000000\n1.300000\n6.700000\n\n\n1220\nZiaire Williams\n20.500000\n49.500000\n17.500000\n18.450000\n2.700000\n6.050000\n0.439500\n0.950000\n3.250000\n...\n0.777500\n0.400000\n1.700000\n2.100000\n0.950000\n0.500000\n0.200000\n0.850000\n1.700000\n6.900000\n\n\n1221\nZion Williamson\n21.000000\n45.000000\n45.000000\n33.100000\n10.100000\n16.600000\n0.609500\n0.200000\n0.650000\n...\n0.706000\n2.350000\n4.750000\n7.100000\n4.150000\n1.000000\n0.600000\n3.050000\n2.200000\n26.500000\n\n\n1223\nÁlex Abrines\n24.000000\n58.000000\n5.333333\n16.533333\n1.766667\n4.666667\n0.381667\n1.266667\n3.533333\n...\n0.889667\n0.266667\n1.200000\n1.433333\n0.533333\n0.500000\n0.133333\n0.433333\n1.700000\n5.333333\n\n\n1225\nÖmer Aşık\n30.400000\n27.000000\n16.600000\n13.360000\n0.800000\n1.760000\n0.438000\n0.000000\n0.000000\n...\n0.355200\n0.960000\n2.840000\n3.820000\n0.300000\n0.200000\n0.280000\n0.660000\n1.400000\n2.040000\n\n\n\n\n905 rows × 27 columns\n\n\n\nThere are a few instances of the same player showing multiple times in the dataframe since people were traded and played for different teams throughout the season, so I took the averages of all of their values to create a set of stats for the season for them.\n\ndf=df.drop(columns=['Age', 'G', 'GS', '2P', '3P', 'TRB', 'FT','PF', 'FG', 'MP'], axis=1)\n\nSo now that we have this data, I will take all of the features besides position, age, team, games played, games started (As well as some irrelevant features, i.e. those that are just linear combinations of other features) and will use them to create clusters so we can start to draw some conclusions. We should feel pretty good about modeling the clusters as gaussian since there are over eight hundred players that played in 2022 which means we should feel alright about assuming normality across each predictor, especially since we have data across 6 seasons.\n\nn_components = np.arange(1, 15)\nmodels = [GaussianMixture(n, covariance_type='full', random_state=0).fit(X)\n          for n in n_components]\n\nplt.plot(n_components, [m.bic(X) for m in models], label='BIC')\nplt.plot(n_components, [m.aic(X) for m in models], label='AIC')\nplt.legend(loc='best')\nplt.xlabel('n_components');\n\n\n\n\nThis code here, uses metrics Bayesian Information Criterion (BIC) and Aikake Information Criterion (AIC), to see which number of clusters would best tell us about the data. The lower the value the better. Both of the metrics are based on the likelihood function for the potential mixture models and the main difference between them is that BIC punishes models with more parameters more than AIC, as we can see from the plot, since the BIC is minimized between 4 and 5 and AIC continues to decrease as the number of components reaches 50. The AIC is equal to \\(2k-2\\ln(L)\\) and the BIC is equal to \\(k\\ln(n)-2\\ln(L)\\) where \\(k\\) is the number of parameters. So as the likelihood that the model proposed (dependent on the number of clusters) has a higher likelihood of explaining the data, the AIC and BIC both decrease.\nThis graph essentially says that the model that explains the data the best without overfitting is between 4 and 5 clusters, since that is where BIC is at a minimum.\n\nmodel = GaussianMixture(n_components=4, random_state=0).fit(X)\n\n\npreds = pd.Series(model.predict(X))\nX[\"Cluster\"] = preds\nX.dropna()\n\n\n\n\n\n\n\n\nFGA\nFG%\n3PA\n3P%\n2PA\n2P%\neFG%\nFTA\nFT%\nORB\nDRB\nAST\nSTL\nBLK\nTOV\nPTS\nCluster\n\n\n\n\n3\n7.400000\n0.465000\n3.600000\n0.390000\n3.800000\n0.536000\n0.560000\n0.700000\n0.894000\n0.500000\n1.600000\n1.000000\n0.600000\n0.200000\n0.600000\n8.900000\n3.0\n\n\n4\n4.533333\n0.403333\n1.900000\n0.362333\n2.633333\n0.433667\n0.480000\n0.600000\n0.764333\n0.266667\n0.766667\n1.700000\n0.333333\n0.066667\n0.833333\n4.800000\n0.0\n\n\n5\n10.966667\n0.477111\n3.555556\n0.325222\n7.422222\n0.544667\n0.529222\n3.222222\n0.683556\n1.711111\n4.544444\n2.733333\n0.744444\n0.688889\n1.644444\n13.777778\n1.0\n\n\n6\n3.133333\n0.179333\n1.900000\n0.169667\n1.233333\n0.202333\n0.227667\n0.966667\n0.560667\n0.200000\n1.100000\n0.633333\n0.433333\n0.066667\n0.166667\n2.600000\n3.0\n\n\n8\n5.250000\n0.422333\n1.916667\n0.380333\n3.316667\n0.448000\n0.491833\n1.050000\n0.848333\n0.333333\n1.316667\n2.116667\n0.633333\n0.166667\n0.966667\n6.033333\n3.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n867\n7.400000\n0.437000\n2.500000\n0.353000\n4.900000\n0.480000\n0.497000\n1.000000\n0.905000\n0.100000\n1.600000\n2.400000\n0.300000\n0.100000\n1.400000\n8.200000\n1.0\n\n\n868\n10.050000\n0.435000\n2.650000\n0.314000\n7.400000\n0.477500\n0.476000\n2.100000\n0.756500\n0.400000\n2.650000\n3.950000\n1.500000\n0.450000\n2.150000\n11.150000\n3.0\n\n\n869\n7.480000\n0.521000\n2.700000\n0.314400\n4.800000\n0.590600\n0.578600\n1.120000\n0.859400\n0.280000\n2.020000\n3.760000\n0.800000\n0.140000\n0.720000\n9.360000\n0.0\n\n\n870\n6.844444\n0.633444\n0.133333\n0.107111\n6.711111\n0.644222\n0.635000\n3.222222\n0.661556\n1.711111\n3.033333\n1.366667\n0.466667\n0.655556\n0.933333\n10.888889\n3.0\n\n\n871\n5.328571\n0.438429\n2.314286\n0.326143\n3.014286\n0.522286\n0.508571\n1.942857\n0.777286\n0.671429\n2.657143\n1.100000\n0.442857\n0.328571\n1.000000\n7.214286\n2.0\n\n\n\n\n621 rows × 17 columns\n\n\n\n\nY = X.drop('Cluster', axis = 1)\ncluster_assignments = model.predict(Y)\n\n# Get the cluster centers from the fitted model\ncluster_centers = model.means_  # This will give the centers of each cluster\n\n# Calculate distances between each player and the cluster centers\ndistances_to_centers = np.linalg.norm(Y - cluster_centers[cluster_assignments], axis=1)\n\n# Find the indices of players closest to each cluster center\nclosest_players_indices = np.argmin(distances_to_centers, axis=0)\n\n# Get the details of players closest to each cluster center\nclosest_players = Y.iloc[closest_players_indices]\n\n\nX_numeric = X.select_dtypes(include='number')\n\n# Scale the numeric columns\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_numeric)\n\n# Convert back to DataFrame (keeping the column names)\nX_scaled_df = pd.DataFrame(X_scaled, columns=X_numeric.columns)\n\n# Add the cluster assignments to your scaled data\nX_scaled_df['Cluster'] = cluster_assignments\n\n# Melt the DataFrame to create a long-form dataset for violin plot\nmelted_data_scaled = X_scaled_df.melt(id_vars='Cluster', var_name='Statistic', value_name='Value')\n\n# Plotting separate violin plots for each statistic\ng = sns.FacetGrid(melted_data_scaled, col='Statistic', col_wrap=3, height=4)\ng.map(sns.violinplot, 'Cluster', 'Value', palette='Set3')\ng.set_xticklabels(rotation=45)\ng.set_titles('{col_name}')\nplt.tight_layout()\nplt.show()\n\n/Users/abeporschet/anaconda3/lib/python3.11/site-packages/seaborn/axisgrid.py:712: UserWarning:\n\nUsing the violinplot function without specifying `order` is likely to produce an incorrect plot."
  }
]