{
  "hash": "bf9ceb810453ffb557b0adf46939a31d",
  "result": {
    "markdown": "---\ntitle: \"Gaussian Mixture Models and Expectation-Maximization\"\nauthor: \"Abraham Porschet\"\ndate: \"2023-12-10\"\ncategories: [code, analysis]\nimage: \"image.png\"\n---\n\n# Expectation Maximization\n\nThe *Expectation Maximization (E-M) Algorithm* is an iterative approach to find maximum likelihood estimates for latent variables (since the things we want to maximize are only indirectly available). It is comprised of an estimation step, which tries to estimate the unknown variables, and a maximization step, which then tries to optimize the parameters of the model to better explain the data.\n\nThe unknown parameters are sometimes written as $\\phi$ or $\\Theta$, and we can call the latent, \"nuisance,\" variables $J$, and the observed data $U$. So, from above, the process can be roughly seen as \n$$ \\Theta^* = \\operatorname*{argmax}_{\\Theta} \\sum_{J\\in\\mathcal{J}^n} P(\\Theta, J|U) $$ \nSince this shows us maximizing the posterior probability of parameters $\\Theta$ given our data and we are summing over $J$ in order to marginalize out our latent variables (Dellaert, 2002).\n\nThis process was first rigorously defined on the exponential family, where the probability density functions take the form \n$$ f(x|\\phi) = b(x)\\exp(\\phi t(x)^T/a(\\phi))$$\n\nwhere $\\phi$ is a $1\\times r$ parameter vector and $t(x)$ is a $1\\times r$ vector of sufficient statistics for the data. Our \"natural parameter\" for these exponential distributions is given by some $r\\times r$ linear transformation. \n\nTo run the \\textbf{E-M} algorithm on this example, we first enter the expectation step, and take $t^{(p)}=E[t(x)|y,\\phi^{(p)}]$  with the $(p)$ denoting the $p^{th}$ cycle of the algorithm, trying to estimate the vector of sufficient statistics for the exponential distribution. \n\nThe maximization step, is then taking the equation $E[t(x)|y,\\phi^{(p)}]=t^{(p)}$ and we call the solution to this equation $\\phi^{(p+1)}$. We then plug in $\\phi^{(p+1)}$ to the expectation step and keep iterating. \n\nThis looks nice, and seems like it could work, but if you are anything like me, you might wonder how we decide when to stop. Possibly the coolest part of this algorithm is that it actually converges to a local maximum every time (Dempster *et al.* 1976). Since it always converges to a local maximum, it means that if we \"guess\" a decent parameter space to start off the algorithm, we will converge to what is likely to be a very solid set of estimates.\n\nIn Dempster, Laird, and Rubin's seminal paper *Maximum Likelihood via the 'EM' Algorithm*, they enumerate the process detailed above, proved the convergence, and later on, proposed that E-M could be used to in *finite clusters*. This foreshadowed the most common usage of the algorithm, clustering, or more specifically, dividing unlabeled data into nice clusters. For example, if we know that our raw data is comprised of unique groups represented by different probability distributions, we can use the E-M algorithm to change the parameters for the estimated distributions of these groups to maximize the probability that the data belongs to the proposed clustering.\n\n## Gaussian Mixture Modeling\n\nOne of the most common usages of expectation maximization, and specifically clustering, is *Gaussian Mixture Modeling* (GMM) (Hasselblad 1966). This process is essentially assuming that each group you are trying to sort out is represented by a normal distribution. This is often a very convenient technique to use because things often actually do follow normal distributions because of the central limit theorem (everyone's favorite statistics theorem) and because once we have clusters that are normal, it is much easier to do inference on the clusters and talk about them with people who don't know as much about statistics because gaussian distributions are so well understood.\n\nGMMs are used to observe clusters everywhere. They are used to create customer archetypes in retail, to better understand the different ways people shop, they are used in medical scenarios in order to identify types of tumors for cancer detection. From this little sample of use cases, it is pretty obvious that this is a really powerful (and pretty cool) use for a powerful algorithm.\n\n## Example Code\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n#importing necessary packages\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt \nimport pandas as pd \nfrom numpy import random\n```\n:::\n\n\nTo show how effective this algorithm can be, I am going to make a set of five blobs of data, each with a center, and then I will show how accurately the algorithm can cluster the data into the blobs that created the underlying data.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n#set a random seed so that we actually get clusters that kind of look like separate clusters\nrandom.seed(195)\nx, _ = make_blobs(n_samples=450, centers=5, cluster_std=1.84)\nplt.figure(figsize=(8, 6))\nplt.scatter(x[:,0], x[:,1])\nplt.show() \n```\n\n::: {.cell-output .cell-output-display}\n![](Stat-61-Final-Project_files/figure-html/cell-3-output-1.png){width=652 height=485}\n:::\n:::\n\n\nNow I'm going to fit the algorithm with the prior understanding that the data is made of five clusters of approximately normal data.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ngm = GaussianMixture(n_components=5).fit(x)\n\ngm.get_params() \n\n{'covariance_type': 'full',\n 'init_params': 'kmeans',\n 'max_iter': 100,\n 'means_init': None,\n 'n_components': 5,\n 'n_init': 1,\n 'precisions_init': None,\n 'random_state': None,\n 'reg_covar': 1e-06,\n 'tol': 0.001,\n 'verbose': 0,\n 'verbose_interval': 10,\n 'warm_start': False,\n 'weights_init': None} \n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\n{'covariance_type': 'full',\n 'init_params': 'kmeans',\n 'max_iter': 100,\n 'means_init': None,\n 'n_components': 5,\n 'n_init': 1,\n 'precisions_init': None,\n 'random_state': None,\n 'reg_covar': 1e-06,\n 'tol': 0.001,\n 'verbose': 0,\n 'verbose_interval': 10,\n 'warm_start': False,\n 'weights_init': None}\n```\n:::\n:::\n\n\nThis code here fits the model and lets it learn from the data, in the next plot, I will plot the centers that the data came up with, and then on the plot after that I will plot the boundaries for the clusters that the algorithm came up with.\n\n",
    "supporting": [
      "Stat-61-Final-Project_files"
    ],
    "filters": [],
    "includes": {}
  }
}